{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Sympiler is a domain-specific code generator and library that optimizes sparse matrix computations by decoupling the symbolic analysis phase from the numerical manipulation stage in sparse codes. Symbolic analysis refers to extracting information from location of nonzeros in a sparse matrix. Sympiler includes a set of compile-time and runtime techniques that we provide an overview in this part. Why Sympiler? Sympiler addresses the challenge of irregular memory and indirect memory accesses in sparse computation through efficient use of compile-time and runtime information. Since compile-time analysis in these codes miss on several optimizations, Sympiler also organizes runtime information, particularly for when computation pattern will be reuse due to static sparsity pattern in an application. Overview Sympiler takes an input specification and generates an efficient parallel code using a set of compile-time transformation and also runtime inspection. We provide an overview of input and the Sympiler internals. Input and Output The input code to Sympiler specifies a set of sparse methods and their input matrix and vectors. Code implementing a sparse method is represented in a domain-specific abstract syntax tree (AST). Sympiler produces the output code by applying a series of lowering phases to the AST. Each lowering phase applies a inspector-guided tranformation at compile-time and builds a symbolic inspector associated with the transformation. Compile-time inspector-guided transformations The compile-time flow in Sympiler is shown in Figure, where a series of inspector-guided transformations are applied to the initial AST insided Sympiler. Each of these transformation, when applied, would require runtime information (obtained through runtime inspectors). Enabling these transformations are either directed by the user or chosen based on the input sparsity pattern of the matrix at runtime. Runtime symbolic inspectors Runtime inspectors in Sympiler process location of nonzeros in a sparse matrix to extract information for optimize the code. The extracted information is called symbolic information and the inspectors are called symbolic inspectors. For each class of numerical algorithms with the same symbolic analysis approach, Sympiler uses a specific symbolic inspector to obtain information about the sparsity structure of the input matrix and stores it in an algorithm-specific way for use during executing the transfomred code (obtained at compile-time from here). We classify the used symbolic inspectors based on the numerical method as well as the transformations enabled by the obtained information. For each combination of algorithm and transformation, the symbolic inspector creates an inspection graph from the given sparsity pattern and traverses it during inspection using a specific inspection strategy. The result of the inspection is the inspection set, which contains the result of running the inspector on the inspection graph. Inspection sets are used to guide the transformations in Sympiler. Additional numerical algorithms and transformations can be added to Sympiler, as long as the required inspectors can be described in this manner as well. The current transformations proposed and prototyped in Sympiler are Iteration-space Prunning , Loop Tiling , Loop Fusion , and Vectorization .","title":"Sympiler Overview"},{"location":"#why-sympiler","text":"Sympiler addresses the challenge of irregular memory and indirect memory accesses in sparse computation through efficient use of compile-time and runtime information. Since compile-time analysis in these codes miss on several optimizations, Sympiler also organizes runtime information, particularly for when computation pattern will be reuse due to static sparsity pattern in an application.","title":"Why Sympiler?"},{"location":"#overview","text":"Sympiler takes an input specification and generates an efficient parallel code using a set of compile-time transformation and also runtime inspection. We provide an overview of input and the Sympiler internals.","title":"Overview"},{"location":"#input-and-output","text":"The input code to Sympiler specifies a set of sparse methods and their input matrix and vectors. Code implementing a sparse method is represented in a domain-specific abstract syntax tree (AST). Sympiler produces the output code by applying a series of lowering phases to the AST. Each lowering phase applies a inspector-guided tranformation at compile-time and builds a symbolic inspector associated with the transformation.","title":"Input and Output"},{"location":"#compile-time-inspector-guided-transformations","text":"The compile-time flow in Sympiler is shown in Figure, where a series of inspector-guided transformations are applied to the initial AST insided Sympiler. Each of these transformation, when applied, would require runtime information (obtained through runtime inspectors). Enabling these transformations are either directed by the user or chosen based on the input sparsity pattern of the matrix at runtime.","title":"Compile-time inspector-guided transformations"},{"location":"#runtime-symbolic-inspectors","text":"Runtime inspectors in Sympiler process location of nonzeros in a sparse matrix to extract information for optimize the code. The extracted information is called symbolic information and the inspectors are called symbolic inspectors. For each class of numerical algorithms with the same symbolic analysis approach, Sympiler uses a specific symbolic inspector to obtain information about the sparsity structure of the input matrix and stores it in an algorithm-specific way for use during executing the transfomred code (obtained at compile-time from here). We classify the used symbolic inspectors based on the numerical method as well as the transformations enabled by the obtained information. For each combination of algorithm and transformation, the symbolic inspector creates an inspection graph from the given sparsity pattern and traverses it during inspection using a specific inspection strategy. The result of the inspection is the inspection set, which contains the result of running the inspector on the inspection graph. Inspection sets are used to guide the transformations in Sympiler. Additional numerical algorithms and transformations can be added to Sympiler, as long as the required inspectors can be described in this manner as well. The current transformations proposed and prototyped in Sympiler are Iteration-space Prunning , Loop Tiling , Loop Fusion , and Vectorization .","title":"Runtime symbolic inspectors"},{"location":"benchmark/","text":"To evaluate the performance of the Sympiler's generated code and compare it with other libraries and code generators, we created a set of benchmarks. The Sympiler benchmark compares the performance sparse BLAS kernels generated by Sympiler with other libraries. And the HDagg benchmark compares different aggregation techniques for a group of methods with loop-carried dependence. Sympiler benchmark Sympiler benchmark is available from here and contains scripts and drivers to compare Sympiler with libraries such as MKL and Suitesparse for Cholesky factorization and sparse triangular solver. HDagg benchmark The HDagg benchmark is available from here and compares prior DAG partitioning or DAG scheduling methods with aggregation methods used in Sympiler. All methods are tested for sparse triangular solver, Incomplete LU0, and Incomplete Cholesky. A detailed comparison between aggregation algorithms and the two aggregation algorithms in Sympiler are provided in the HDagg paper .","title":"Benchmarks"},{"location":"benchmark/#sympiler-benchmark","text":"Sympiler benchmark is available from here and contains scripts and drivers to compare Sympiler with libraries such as MKL and Suitesparse for Cholesky factorization and sparse triangular solver.","title":"Sympiler benchmark"},{"location":"benchmark/#hdagg-benchmark","text":"The HDagg benchmark is available from here and compares prior DAG partitioning or DAG scheduling methods with aggregation methods used in Sympiler. All methods are tested for sparse triangular solver, Incomplete LU0, and Incomplete Cholesky. A detailed comparison between aggregation algorithms and the two aggregation algorithms in Sympiler are provided in the HDagg paper .","title":"HDagg benchmark"},{"location":"citation/","text":"Citing us Sympiler If you are using Sympiler or its components, please cite one of the following. @inproceedings{Cheshmi:2017:STS:3126908.3126936, author = {Cheshmi, Kazem and Kamil, Shoaib and Strout, Michelle Mills and Dehnavi, Maryam Mehri}, title = {Sympiler: Transforming Sparse Matrix Codes by Decoupling Symbolic Analysis}, booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis}, series = {SC '17}, year = {2017}, isbn = {978-1-4503-5114-0}, location = {Denver, Colorado}, pages = {13:1--13:13}, articleno = {13}, numpages = {13}, url = {http://doi.acm.org/10.1145/3126908.3126936}, doi = {10.1145/3126908.3126936}, acmid = {3126936}, publisher = {ACM}, address = {New York, NY, USA}, } @phdthesis{cheshmi2022transforming, title={Transforming Sparse Matrix Computations}, author={Cheshmi, Kazem}, publisher = {University of Toronto, Computer Science}, url = {http://hdl.handle.net/1807/123318}, year={2022} } ParSy If you want to compare with tiling techniques used in Sympiler, for LBC/ParSy please cite: @INPROCEEDINGS{cheshmi2018parsy, author={Cheshmi, Kazem and Kamil, Shoaib and Strout, Michelle Mills and Dehnavi, Maryam Mehri}, booktitle={SC18: International Conference for High Performance Computing, Networking, Storage and Analysis}, title={ParSy: Inspection and Transformation of Sparse Matrix Computations for Parallelism}, year={2018}, pages={779-793}, publisher = {IEEE Press}, address = {Piscataway, NJ, USA}, url = {http://dl.acm.org/citation.cfm?id=3291656.3291739}, doi={10.1109/SC.2018.00065}} HDagg For HDagg please cite: @INPROCEEDINGS{9820651, author={Zarebavani, Behrooz and Cheshmi, Kazem and Liu, Bangtian and Strout, Michelle Mills and Dehnavi, Maryam Mehri}, booktitle={2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, title={HDagg: Hybrid Aggregation of Loop-carried Dependence Iterations in Sparse Matrix Computations}, year={2022}, pages={1217-1227}, doi={10.1109/IPDPS53621.2022.00121}} Partially Strided Codelet If you want to compare with vectorization techniques in Sympiler, please cite: @inproceedings{cheshmi22psc, author = {Cheshmi, Kazem and Cetenic, Zachary and Dehnavi, Maryam Mehri}, title = {Vectorizing Sparse Matrix Computations with Partially-Strided Codelets}, year = {2022}, publisher = {IEEE Press}, booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis}, keywords = {matrix computations, domain-specific code generation, loop transformations, parallel algorithms}, location = {Dallas, Texas}, series = {SC \u201922} } Sparse Fusion If you want to compare with fusion techniques in Sympiler, please cite one of the following: @inproceedings{cheshmi22fusion, author = {Cheshmi, Kazem and Strout, Michelle Mills and Dehnavi, Maryam Mehri}, title = {Optimizing Sparse Computations Jointly}, year = {2022}, isbn = {9781450392044}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3503221.3508439}, doi = {10.1145/3503221.3508439}, booktitle = {Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}, pages = {459\u2013460}, numpages = {2}, keywords = {loop-carried dependence, sparse matrix code, loop fusion}, location = {Seoul, Republic of Korea}, series = {PPoPP '22} } @article{cheshmi2021composing, title={Composing Loop-carried Dependence with Other Loops}, author={Cheshmi, Kazem and Strout, Michelle Mills and Dehnavi, Maryam Mehri}, journal={arXiv preprint arXiv:2111.12238}, year={2021} } We are keen to hear your success stories with Sympiler. Please contact us . Credits The people have been involved in the development of Sympiler (so far): Kazem Cheshmi Behrooz Zarebavani Shoaib Kamil Michelle Mills Strout Maryam Mehri Dehnavi","title":"Publications"},{"location":"citation/#citing-us","text":"","title":"Citing us"},{"location":"citation/#sympiler","text":"If you are using Sympiler or its components, please cite one of the following. @inproceedings{Cheshmi:2017:STS:3126908.3126936, author = {Cheshmi, Kazem and Kamil, Shoaib and Strout, Michelle Mills and Dehnavi, Maryam Mehri}, title = {Sympiler: Transforming Sparse Matrix Codes by Decoupling Symbolic Analysis}, booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis}, series = {SC '17}, year = {2017}, isbn = {978-1-4503-5114-0}, location = {Denver, Colorado}, pages = {13:1--13:13}, articleno = {13}, numpages = {13}, url = {http://doi.acm.org/10.1145/3126908.3126936}, doi = {10.1145/3126908.3126936}, acmid = {3126936}, publisher = {ACM}, address = {New York, NY, USA}, } @phdthesis{cheshmi2022transforming, title={Transforming Sparse Matrix Computations}, author={Cheshmi, Kazem}, publisher = {University of Toronto, Computer Science}, url = {http://hdl.handle.net/1807/123318}, year={2022} }","title":"Sympiler"},{"location":"citation/#parsy","text":"If you want to compare with tiling techniques used in Sympiler, for LBC/ParSy please cite: @INPROCEEDINGS{cheshmi2018parsy, author={Cheshmi, Kazem and Kamil, Shoaib and Strout, Michelle Mills and Dehnavi, Maryam Mehri}, booktitle={SC18: International Conference for High Performance Computing, Networking, Storage and Analysis}, title={ParSy: Inspection and Transformation of Sparse Matrix Computations for Parallelism}, year={2018}, pages={779-793}, publisher = {IEEE Press}, address = {Piscataway, NJ, USA}, url = {http://dl.acm.org/citation.cfm?id=3291656.3291739}, doi={10.1109/SC.2018.00065}}","title":"ParSy"},{"location":"citation/#hdagg","text":"For HDagg please cite: @INPROCEEDINGS{9820651, author={Zarebavani, Behrooz and Cheshmi, Kazem and Liu, Bangtian and Strout, Michelle Mills and Dehnavi, Maryam Mehri}, booktitle={2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, title={HDagg: Hybrid Aggregation of Loop-carried Dependence Iterations in Sparse Matrix Computations}, year={2022}, pages={1217-1227}, doi={10.1109/IPDPS53621.2022.00121}}","title":"HDagg"},{"location":"citation/#partially-strided-codelet","text":"If you want to compare with vectorization techniques in Sympiler, please cite: @inproceedings{cheshmi22psc, author = {Cheshmi, Kazem and Cetenic, Zachary and Dehnavi, Maryam Mehri}, title = {Vectorizing Sparse Matrix Computations with Partially-Strided Codelets}, year = {2022}, publisher = {IEEE Press}, booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage, and Analysis}, keywords = {matrix computations, domain-specific code generation, loop transformations, parallel algorithms}, location = {Dallas, Texas}, series = {SC \u201922} }","title":"Partially Strided Codelet"},{"location":"citation/#sparse-fusion","text":"If you want to compare with fusion techniques in Sympiler, please cite one of the following: @inproceedings{cheshmi22fusion, author = {Cheshmi, Kazem and Strout, Michelle Mills and Dehnavi, Maryam Mehri}, title = {Optimizing Sparse Computations Jointly}, year = {2022}, isbn = {9781450392044}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3503221.3508439}, doi = {10.1145/3503221.3508439}, booktitle = {Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}, pages = {459\u2013460}, numpages = {2}, keywords = {loop-carried dependence, sparse matrix code, loop fusion}, location = {Seoul, Republic of Korea}, series = {PPoPP '22} } @article{cheshmi2021composing, title={Composing Loop-carried Dependence with Other Loops}, author={Cheshmi, Kazem and Strout, Michelle Mills and Dehnavi, Maryam Mehri}, journal={arXiv preprint arXiv:2111.12238}, year={2021} } We are keen to hear your success stories with Sympiler. Please contact us .","title":"Sparse Fusion"},{"location":"citation/#credits","text":"The people have been involved in the development of Sympiler (so far): Kazem Cheshmi Behrooz Zarebavani Shoaib Kamil Michelle Mills Strout Maryam Mehri Dehnavi","title":"Credits"},{"location":"fusion/","text":"Numerical algorithms and optimization methods are typically composed of numerous consecutive sparse matrix computations. For example, in iterative solvers such as Krylov methods, sparse kernels that apply a preconditioner or update the residual are repeatedly executed inside and between iterations of the solver. Separately optimizing such kernels creates synchronization and load-imbalance between threads. Also, opportunities for data reuse between two sparse computations might not be realized when sparse kernels are optimized separately. Sympiler uses sparse fusion to fuse sparse methods for improving locality and thread-level parallelism. Sparse fusion has a transformation and an inspector. Transformation To generate the fused code, a fused transformation is applied to the initial AST at compile-time and two variants of the fused code are generated, shown below. The transformation variants are separated and interleaved . The fused code uses a reuse ratio at runtime to select the correct variant for the specific input. The reuse ratio ( reuse_ratio ) specifies how much common data between the two loops exist. The variable fusion in line 2 of and the two fused variants is set to False if the inspector determines fusion is not profitable. The separated variant is selected when the reuse ratio is smaller than one. In this variant, iterations of one of the loops run consecutively without checking the loop type. The interleaved variant is chosen when the reuse ratio is larger than one. In this variant, iterations of both loops should run interleaved, and the variant checks the loop type per iteration. /// The input AST to the fusion transformation Fuse:for(I1){//loop 1 ... for(In) x[h(I1,...,In)] = a*y[g(I1,...,In)]; } Fuse:for(J1){//loop 2 ... for(Jm) z[h\u2019(J1,...,Jm)] = a*x[g\u2019(J1,...,Jm)]; } /// Separated variant of the fusion transformation if(FusedSchedule.fusion && reuse_ratio < 1){ for (every s-partition s){ #pragma omp parallel for for (every w-partition w){ for(v \u2208 FusedSchedule[s][w].L1){//loop 1 ... for(In) x[h(v,...,In)] = a*y[g(v,...,In)]; } for(v \u2208 FusedSchedule[s][w].L2){//loop 2 ... for(Jm) z[h\u2019(v,...,Jm)] = a*x[g\u2019(v,...,Jm)]; } } } } /// Interleaved variant of the fusion transformation if(FusedSchedule.fusion && reuse_ratio >= 1){ for (every s-partition s){ #pragma omp parallel for for (every w-partition w){ for(v \u2208 FusedSchedule[s][w]){ if(v.type == L1){//loop 1 for(In) x[h(v.id,...,In)] = a*y[g(v,...,In)]; } else {//loop 2 for(Jm) z[h\u2019(v.id,...,Jm)] = a*x[g\u2019(v,...,Jm)]; } } } } } Inspector Sparse fusion uses the multi-sparse DAG partitioning (MSP) algorithm to create an efficient fused partitioning that will be used to schedule iterations of the fused code. MSP partitions vertices of the DAGs of the two input kernels to create parallel load-balanced workloads for all cores while improving locality within each thread. The inputs to MSP are the dependency matrix between sparse methods, the DAG of each method, a reuse ratio. Sparse fusion analyzes the code of both methods, available from its AST, to generate inspector components that create these inputs. MSP takes the inputs and goes through three steps of (1) vertex partitioning and partition pairing with the objective to aggregate iterations without joining the DAGs of the inputs kernels; (2) merging and slack vertex assignment to reduce synchronization and to balance workloads; and (3) packing to improve locality. A detailed explanation of sparse fuision is provided in these references .","title":"Loop Fusion"},{"location":"fusion/#transformation","text":"To generate the fused code, a fused transformation is applied to the initial AST at compile-time and two variants of the fused code are generated, shown below. The transformation variants are separated and interleaved . The fused code uses a reuse ratio at runtime to select the correct variant for the specific input. The reuse ratio ( reuse_ratio ) specifies how much common data between the two loops exist. The variable fusion in line 2 of and the two fused variants is set to False if the inspector determines fusion is not profitable. The separated variant is selected when the reuse ratio is smaller than one. In this variant, iterations of one of the loops run consecutively without checking the loop type. The interleaved variant is chosen when the reuse ratio is larger than one. In this variant, iterations of both loops should run interleaved, and the variant checks the loop type per iteration. /// The input AST to the fusion transformation Fuse:for(I1){//loop 1 ... for(In) x[h(I1,...,In)] = a*y[g(I1,...,In)]; } Fuse:for(J1){//loop 2 ... for(Jm) z[h\u2019(J1,...,Jm)] = a*x[g\u2019(J1,...,Jm)]; } /// Separated variant of the fusion transformation if(FusedSchedule.fusion && reuse_ratio < 1){ for (every s-partition s){ #pragma omp parallel for for (every w-partition w){ for(v \u2208 FusedSchedule[s][w].L1){//loop 1 ... for(In) x[h(v,...,In)] = a*y[g(v,...,In)]; } for(v \u2208 FusedSchedule[s][w].L2){//loop 2 ... for(Jm) z[h\u2019(v,...,Jm)] = a*x[g\u2019(v,...,Jm)]; } } } } /// Interleaved variant of the fusion transformation if(FusedSchedule.fusion && reuse_ratio >= 1){ for (every s-partition s){ #pragma omp parallel for for (every w-partition w){ for(v \u2208 FusedSchedule[s][w]){ if(v.type == L1){//loop 1 for(In) x[h(v.id,...,In)] = a*y[g(v,...,In)]; } else {//loop 2 for(Jm) z[h\u2019(v.id,...,Jm)] = a*x[g\u2019(v,...,Jm)]; } } } } }","title":"Transformation"},{"location":"fusion/#inspector","text":"Sparse fusion uses the multi-sparse DAG partitioning (MSP) algorithm to create an efficient fused partitioning that will be used to schedule iterations of the fused code. MSP partitions vertices of the DAGs of the two input kernels to create parallel load-balanced workloads for all cores while improving locality within each thread. The inputs to MSP are the dependency matrix between sparse methods, the DAG of each method, a reuse ratio. Sparse fusion analyzes the code of both methods, available from its AST, to generate inspector components that create these inputs. MSP takes the inputs and goes through three steps of (1) vertex partitioning and partition pairing with the objective to aggregate iterations without joining the DAGs of the inputs kernels; (2) merging and slack vertex assignment to reduce synchronization and to balance workloads; and (3) packing to improve locality. A detailed explanation of sparse fuision is provided in these references .","title":"Inspector"},{"location":"getting-started-sympiler/","text":"We explain how you can build Sympiler and its iteration DAG partitioner from source. Building Sympiler Pre-requisites CMake C/C++ compiler (gcc, icc, or clang) Dependencies handled by CMake: OpenMP : OpenMP is an optional dependency but without OpenMP, TBB is used internally. METIS : METIS dependency is already handled by CMake. a BLAS Library : (MKL or OpenBLAS) is required Installation Please use the following: git clone --recursive https://github.com/sympiler/sympiler.git cd sympiler mkdir build cd build cmake -DCMAKE_BUILD_TYPE = Release .. make Building DAG partitioners: Pre-requisites CMake C/C++ compiler (gcc, icc, or clang) Dependencies handled by CMake: METIS : METIS dependency is already handled by CMake. Installation If the installation paths of these libraries are in the system path, CMake should be able to handle dependencies. If not, you should set CMake variables as shown below: git clone --recursive https://github.com/sympiler/sympiler.git cd sympiler mkdir build cd build cmake -DCMAKE_BUILD_TYPE = Release .. make How to use sympiler? Sympiler can be used as a code generator or as a library. The DAG partitioning algorithm used inside sympiler can also be used independently. C++ API interface is here .","title":"Getting Started with Sympiler"},{"location":"getting-started-sympiler/#building-sympiler","text":"","title":"Building Sympiler"},{"location":"getting-started-sympiler/#pre-requisites","text":"CMake C/C++ compiler (gcc, icc, or clang) Dependencies handled by CMake: OpenMP : OpenMP is an optional dependency but without OpenMP, TBB is used internally. METIS : METIS dependency is already handled by CMake. a BLAS Library : (MKL or OpenBLAS) is required","title":"Pre-requisites"},{"location":"getting-started-sympiler/#installation","text":"Please use the following: git clone --recursive https://github.com/sympiler/sympiler.git cd sympiler mkdir build cd build cmake -DCMAKE_BUILD_TYPE = Release .. make","title":"Installation"},{"location":"getting-started-sympiler/#building-dag-partitioners","text":"","title":"Building DAG partitioners:"},{"location":"getting-started-sympiler/#pre-requisites_1","text":"CMake C/C++ compiler (gcc, icc, or clang) Dependencies handled by CMake: METIS : METIS dependency is already handled by CMake.","title":"Pre-requisites"},{"location":"getting-started-sympiler/#installation_1","text":"If the installation paths of these libraries are in the system path, CMake should be able to handle dependencies. If not, you should set CMake variables as shown below: git clone --recursive https://github.com/sympiler/sympiler.git cd sympiler mkdir build cd build cmake -DCMAKE_BUILD_TYPE = Release .. make","title":"Installation"},{"location":"getting-started-sympiler/#how-to-use-sympiler","text":"Sympiler can be used as a code generator or as a library. The DAG partitioning algorithm used inside sympiler can also be used independently. C++ API interface is here .","title":"How to use sympiler?"},{"location":"prune/","text":"Variable Iteration Space Pruning (VI-Prune) prunes the iteration space of a loop using information about the sparse computation. The iteration space for sparse codes can be considerably smaller than that for dense codes, since the computation needs to only consider iterations with nonzeros. The inspection stage of Sympiler generates an inspection set that enables transforming the unoptimized sparse code to a code with a reduced iteration space. Transformation The VI-Prune transformation can be applied at a particular loop-level to the sparse code to transform it as shown below. The loop that should be pruned is annotated with PRUNE internally. In the transformed code the iteration space is pruned to pruneSetSize , which is the inspection set size. In addition to the new loop, all references to Ik (the loop index before transformation) are replaced by its corresponding value from the inspection set, pruneSet[Ik]. /// Input code (as an internal AST) for(I1){ . . Prune: for(Ik < m) { . . for(In (Ik , ..., In\u22121)) { a[idx(I1,...,Ik ,...,In )]; } } } /// After applying the VI-Prune transformation for(I1){ . . for(Ik < pruneSetSize) { J = pruneSet[Ik]; . . for(In (J,..., In\u22121)) { a[idx(I1,...,J,...,In )]; } } } Inspector The VI-Prune inspector, the dependency graph of loop is traversed using depth first search (DFS) to determine the new pruned iteration space, i.e. pruneSet and pruneSetSize . The dependence graph computation can be either done with code analysis or using domain information. Sympiler uses domain information, such as knowing the sparse method, to create the dependence graph. More information and examples are available from here","title":"Iteration-space Pruning"},{"location":"prune/#transformation","text":"The VI-Prune transformation can be applied at a particular loop-level to the sparse code to transform it as shown below. The loop that should be pruned is annotated with PRUNE internally. In the transformed code the iteration space is pruned to pruneSetSize , which is the inspection set size. In addition to the new loop, all references to Ik (the loop index before transformation) are replaced by its corresponding value from the inspection set, pruneSet[Ik]. /// Input code (as an internal AST) for(I1){ . . Prune: for(Ik < m) { . . for(In (Ik , ..., In\u22121)) { a[idx(I1,...,Ik ,...,In )]; } } } /// After applying the VI-Prune transformation for(I1){ . . for(Ik < pruneSetSize) { J = pruneSet[Ik]; . . for(In (J,..., In\u22121)) { a[idx(I1,...,J,...,In )]; } } }","title":"Transformation"},{"location":"prune/#inspector","text":"The VI-Prune inspector, the dependency graph of loop is traversed using depth first search (DFS) to determine the new pruned iteration space, i.e. pruneSet and pruneSetSize . The dependence graph computation can be either done with code analysis or using domain information. Sympiler uses domain information, such as knowing the sparse method, to create the dependence graph. More information and examples are available from here","title":"Inspector"},{"location":"sympiler-lib/","text":"The sympiler library is built from the sympiler generated code and has a collection of inspector/executor routines for wide range of sparse linear algebra kernels. Here we explain C++ interfaces of the sympiler library. Sparse BLAS Sparse lower triangular solver (SpTRSV): Description: solving linear system Lx=b Lx=b where L L is a sparse lower triangular matrix and x x and b b are dense vectors. Inspector: inputs, outputs, description Executor: inputs, outputs, description Sparse Cholesky factorization Description: Inspector: Executor: Sparse incomplete LU (SpILU0) Sparse incomplete Cholesky (SpIC0) Sparse matrix-vector multiplication (SpMV) Tiling and Aggregation Load-balanced wavefront coarsening Description Inputs outputs HDagg Description Inputs outputs Codelet mining Coming soon. Fusion Not public yet.","title":"Sympiler Library"},{"location":"sympiler-lib/#sparse-blas","text":"","title":"Sparse BLAS"},{"location":"sympiler-lib/#sparse-lower-triangular-solver-sptrsv","text":"Description: solving linear system Lx=b Lx=b where L L is a sparse lower triangular matrix and x x and b b are dense vectors. Inspector: inputs, outputs, description Executor: inputs, outputs, description","title":"Sparse lower triangular solver (SpTRSV):"},{"location":"sympiler-lib/#sparse-cholesky-factorization","text":"Description: Inspector: Executor:","title":"Sparse Cholesky factorization"},{"location":"sympiler-lib/#sparse-incomplete-lu-spilu0","text":"","title":"Sparse incomplete LU (SpILU0)"},{"location":"sympiler-lib/#sparse-incomplete-cholesky-spic0","text":"","title":"Sparse incomplete Cholesky (SpIC0)"},{"location":"sympiler-lib/#sparse-matrix-vector-multiplication-spmv","text":"","title":"Sparse matrix-vector multiplication (SpMV)"},{"location":"sympiler-lib/#tiling-and-aggregation","text":"","title":"Tiling and Aggregation"},{"location":"sympiler-lib/#load-balanced-wavefront-coarsening","text":"Description Inputs outputs","title":"Load-balanced wavefront coarsening"},{"location":"sympiler-lib/#hdagg","text":"Description Inputs outputs","title":"HDagg"},{"location":"sympiler-lib/#codelet-mining","text":"Coming soon.","title":"Codelet mining"},{"location":"sympiler-lib/#fusion","text":"Not public yet.","title":"Fusion"},{"location":"sympyler/","text":"Sympiler library python interface Tiling/Aggregation Fusion Sparse BLAS","title":"Sympiler library python interface"},{"location":"sympyler/#sympiler-library-python-interface","text":"","title":"Sympiler library python interface"},{"location":"sympyler/#tilingaggregation","text":"","title":"Tiling/Aggregation"},{"location":"sympyler/#fusion","text":"","title":"Fusion"},{"location":"sympyler/#sparse-blas","text":"","title":"Sparse BLAS"},{"location":"tiling/","text":"Sympiler aggregates iterations of loop to create balanced thread-level parallelism and improve locality within a thread. In addition to iteration aggregation, Sympiler also uses block-tiling for some sparse methods to enable locality and vectorization. Aggregation Tiling is performed in Sympiler using a Hierarchical wavefront transformation with an aggregation inspector. Sympiler has two aggregation inspector, LBC and HDagg. Transformation (or Hierarchical wavefront transformation) The parallel tile transformation, also named as hierarchical wavefront(level), tiles a specified loop to use the tiling information provided at runtime. The two code snippets below show before and after the parallel tiling transformation. The loop in line 2 of the code before transformation is changed to lines 2\u20135 in the parallel tiled code. After transformation, all operations and indices that use I1 , which is the index of the transformed loop, will be replaced with a proper value from HLevelSet . The parallel pragma in line 2 ensures that all w-partitions within an l-partition run in parallel. Note that some algorithms may require atomic pragmas, as shown in line 9 of the parallel tiled code. /// Input code (as an internal AST) Tile: for(I1){ . . . for(In(I1)){ Atomic: c /= a[idx(I1,...,In)]; } } /// Tiled code after application of the parallel tiling transformation for( every l\u2212partition i ) { #pragma omp parallel for private (pVars) for( every w\u2212partition j ) { for ( every v \u2208 HLevelSet[i][j] ) { I1 = v ; . . . for( In(I1) ) { #pragma omp atomic c /= a [ idx( I1, ... , In ) ] ; } } } } Inspectors There are two major aggregation algorithms implemented in Sympiler, Load-Balanced wavefront(level) Coarsening (LBC) and Hierarchical Iteration DAG Aggregation (HDagg). Load balance level coarsening (LBC) The goal of Load-Balanced Level Coarsening is to find a set of l-partitions, and within each l-partition, to find a set of disjoint w-partitions with as balanced cost as possible. For improved performance, these partitions adhere to additional constraints to reduce synchronization between threads and maintain load balance. Additionally, there are objective functions for minimizing communication between threads and the number of synchronizations between levels. The LBC is designed for chordal DAGs (A DAG where each vertex is part of a cycle of at most length three) and trees. A detailed discussion of the algorithm is provided in the ParSy paper . Hierarchical Iteration DAG Aggregation (HDagg) The HDagg algorithm statically partitions the DAG of a sparse matrix computation using a hybrid approach during inspection. Its objective is to create a partitioning of a general DAG (DAGs that do not necessarily have a tree structure) that when executed provides a good load balance and low synchronization cost while improving locality. HDagg first groups vertices that form densely connected regions inside the DAG to ensure they execute on the same thread to improve locality. As a result, a grouped DAG is created where each vertex corresponds to a group of vertices. Then, to reduce synchronization costs and to improve load balance and locality, HDagg coarsens wavefronts of the grouped DAG. A detailed discussion of the algorithm is provided in the HDagg paper . Block-tiling Sympiler's block-tiling strategy (also known as 2D Variable-Sized Blocking) converts a sparse code to a set of non-uniform dense sub- kernels. In contrast to the conventional approach of blocking/tiling dense codes, where the input and computations are blocked into smaller uniform sub-kernels, the unstructured computations and inputs in sparse kernels make blocking optimizations challenging. This enables vectorization through calling BLAS kernels. Block-tiling has a transformation and an inspector. transformation The block-tiling transformation is shown in the code below. The inner loop in line 3 transforms to two nested loops (lines 3\u20137) that iterate over the block specified by the outer loop. The tile-blocking code transformation is domain specific and the general H-Level transformation can not be used. /// The input AST to the block-tiling transformation block-tile: for(I) { for(J) { B[idx1(I,J)] += a[idx2(I,J)]; } } /// The block-tiled code after transformation for(b < blockSetSize) { for(J1 < blockSet[b].x) { for(J2 < blockSet [b].y) { B[idx1(b,J1,J2)] += A[idx2(b, J1, J2)]; } } } inspector The symbolic inspector identifies sub-kernels with similar structure in the sparse matrix methods and the sparse inputs to provide the VS-Block stage with blockable sets that are not necessarily of the same size or consecutively located. These blocks are similar to the concept of supernodes (consecutive columns with the same pattern) in sparse libraries. Examples for the tile-blocking transformation are provided in the Sympiler paper .","title":"Loop Tiling/Aggregation"},{"location":"tiling/#aggregation","text":"Tiling is performed in Sympiler using a Hierarchical wavefront transformation with an aggregation inspector. Sympiler has two aggregation inspector, LBC and HDagg.","title":"Aggregation"},{"location":"tiling/#transformation-or-hierarchical-wavefront-transformation","text":"The parallel tile transformation, also named as hierarchical wavefront(level), tiles a specified loop to use the tiling information provided at runtime. The two code snippets below show before and after the parallel tiling transformation. The loop in line 2 of the code before transformation is changed to lines 2\u20135 in the parallel tiled code. After transformation, all operations and indices that use I1 , which is the index of the transformed loop, will be replaced with a proper value from HLevelSet . The parallel pragma in line 2 ensures that all w-partitions within an l-partition run in parallel. Note that some algorithms may require atomic pragmas, as shown in line 9 of the parallel tiled code. /// Input code (as an internal AST) Tile: for(I1){ . . . for(In(I1)){ Atomic: c /= a[idx(I1,...,In)]; } } /// Tiled code after application of the parallel tiling transformation for( every l\u2212partition i ) { #pragma omp parallel for private (pVars) for( every w\u2212partition j ) { for ( every v \u2208 HLevelSet[i][j] ) { I1 = v ; . . . for( In(I1) ) { #pragma omp atomic c /= a [ idx( I1, ... , In ) ] ; } } } }","title":"Transformation (or Hierarchical wavefront transformation)"},{"location":"tiling/#inspectors","text":"There are two major aggregation algorithms implemented in Sympiler, Load-Balanced wavefront(level) Coarsening (LBC) and Hierarchical Iteration DAG Aggregation (HDagg).","title":"Inspectors"},{"location":"tiling/#load-balance-level-coarsening-lbc","text":"The goal of Load-Balanced Level Coarsening is to find a set of l-partitions, and within each l-partition, to find a set of disjoint w-partitions with as balanced cost as possible. For improved performance, these partitions adhere to additional constraints to reduce synchronization between threads and maintain load balance. Additionally, there are objective functions for minimizing communication between threads and the number of synchronizations between levels. The LBC is designed for chordal DAGs (A DAG where each vertex is part of a cycle of at most length three) and trees. A detailed discussion of the algorithm is provided in the ParSy paper .","title":"Load balance level coarsening (LBC)"},{"location":"tiling/#hierarchical-iteration-dag-aggregation-hdagg","text":"The HDagg algorithm statically partitions the DAG of a sparse matrix computation using a hybrid approach during inspection. Its objective is to create a partitioning of a general DAG (DAGs that do not necessarily have a tree structure) that when executed provides a good load balance and low synchronization cost while improving locality. HDagg first groups vertices that form densely connected regions inside the DAG to ensure they execute on the same thread to improve locality. As a result, a grouped DAG is created where each vertex corresponds to a group of vertices. Then, to reduce synchronization costs and to improve load balance and locality, HDagg coarsens wavefronts of the grouped DAG. A detailed discussion of the algorithm is provided in the HDagg paper .","title":"Hierarchical Iteration DAG Aggregation (HDagg)"},{"location":"tiling/#block-tiling","text":"Sympiler's block-tiling strategy (also known as 2D Variable-Sized Blocking) converts a sparse code to a set of non-uniform dense sub- kernels. In contrast to the conventional approach of blocking/tiling dense codes, where the input and computations are blocked into smaller uniform sub-kernels, the unstructured computations and inputs in sparse kernels make blocking optimizations challenging. This enables vectorization through calling BLAS kernels. Block-tiling has a transformation and an inspector.","title":"Block-tiling"},{"location":"tiling/#transformation","text":"The block-tiling transformation is shown in the code below. The inner loop in line 3 transforms to two nested loops (lines 3\u20137) that iterate over the block specified by the outer loop. The tile-blocking code transformation is domain specific and the general H-Level transformation can not be used. /// The input AST to the block-tiling transformation block-tile: for(I) { for(J) { B[idx1(I,J)] += a[idx2(I,J)]; } } /// The block-tiled code after transformation for(b < blockSetSize) { for(J1 < blockSet[b].x) { for(J2 < blockSet [b].y) { B[idx1(b,J1,J2)] += A[idx2(b, J1, J2)]; } } }","title":"transformation"},{"location":"tiling/#inspector","text":"The symbolic inspector identifies sub-kernels with similar structure in the sparse matrix methods and the sparse inputs to provide the VS-Block stage with blockable sets that are not necessarily of the same size or consecutively located. These blocks are similar to the concept of supernodes (consecutive columns with the same pattern) in sparse libraries. Examples for the tile-blocking transformation are provided in the Sympiler paper .","title":"inspector"},{"location":"vect/","text":"Irregular computations, such as in sparse matrix codes, frequently appear in scientific and machine learning problems. The performance of these applications is noticeably improved if their code is vectorized to exploit single instruction mul- tiple data (SIMD) capabilities of the underlying architecture. Vectorization potentially increases opportunities to optimize for locality, further increasing performance. SIMD instructions can efficiently vectorize groups of operations that access consecutive data, i.e. have a strided access pattern. Sympiler's vectorization transformation creates a set of codelets per an operation in the code and then uses a mining algorithm as its inspector to find codelets at runtime. A codelet is a polyhedral model for a group of operations where each operation has three access functions. Codelets are classified into strided codelets (or BLAS) and partially strided codelets. These codelets differ on how many strided access functions they have. BLAS codelets have three strided access functions and partially strided codelets have one or two strided access functions. This classification enables performing any sparse method with a handful of codelets. Transformation Sympiler transforms the code so the mined codelets at runtime are mapped to the proper codelet type. Each codelet type, PSC or BLAS, is implemented with a parameterized vectorized routine that efficiently uses the SIMD instructions of the target architecture, i.e. x86. The parameterization also allows us to generate concise code invariant to the number of codelets that are mined for a set of operations. The parameterized vectorized routine takes the iteration space and access functions of a codelet and data spaces of the kernel as input and vectorizes all operations in the codelet. The cpde below shows how a switch-case statement is used to map a mined codelet from clist to one of the three parametrized codes. The number of parameterized code per codelet types can increase for better efficiency. /// Codelet mapping transformation for sparse method with one operation such as SpMV #include \"Codeletsx86.h\" void vectorized_code(vector<Codlet*> clsit, CSR* A, double *x, double *y){ for(int i = 0; i < clist.partitions(); i++){ #pragma omp parallel for(int j = 0; j < clist[i].size(); j++){ switch(clist[i][j].type){ case BLAS: BLAS_codelet(); break; case PSCI: PSCI_codelet(); break; case PSCII: PSCII_codelet(); break; } } } } Inspector Sympiler's vectorization inspector has a codelet mining algorithm that creates a list of codelets from access functions and the iteration space of a sparse kernel with the objective to minimize the overall cost of the final codelet list. The list is computed at runtime and provided to the transformed code. The codelet mining uses a locality-based mining (LCM) algorithm that mines operations and finds an efficient codelet combination that satisfies the correctness of the vectorized code and minimizes the total cost of codelets. The LCM algorithm has two steps; in the first step, LCM permutes operations to increase data reuse and the possibility of creating efficient codelets from that list of operations. This step also groups operations that have the most reuse into computation regions. The second step of LCM searches for efficient codelets in consecutive operations of each computation region.","title":"Vectorization"},{"location":"vect/#transformation","text":"Sympiler transforms the code so the mined codelets at runtime are mapped to the proper codelet type. Each codelet type, PSC or BLAS, is implemented with a parameterized vectorized routine that efficiently uses the SIMD instructions of the target architecture, i.e. x86. The parameterization also allows us to generate concise code invariant to the number of codelets that are mined for a set of operations. The parameterized vectorized routine takes the iteration space and access functions of a codelet and data spaces of the kernel as input and vectorizes all operations in the codelet. The cpde below shows how a switch-case statement is used to map a mined codelet from clist to one of the three parametrized codes. The number of parameterized code per codelet types can increase for better efficiency. /// Codelet mapping transformation for sparse method with one operation such as SpMV #include \"Codeletsx86.h\" void vectorized_code(vector<Codlet*> clsit, CSR* A, double *x, double *y){ for(int i = 0; i < clist.partitions(); i++){ #pragma omp parallel for(int j = 0; j < clist[i].size(); j++){ switch(clist[i][j].type){ case BLAS: BLAS_codelet(); break; case PSCI: PSCI_codelet(); break; case PSCII: PSCII_codelet(); break; } } } }","title":"Transformation"},{"location":"vect/#inspector","text":"Sympiler's vectorization inspector has a codelet mining algorithm that creates a list of codelets from access functions and the iteration space of a sparse kernel with the objective to minimize the overall cost of the final codelet list. The list is computed at runtime and provided to the transformed code. The codelet mining uses a locality-based mining (LCM) algorithm that mines operations and finds an efficient codelet combination that satisfies the correctness of the vectorized code and minimizes the total cost of codelets. The LCM algorithm has two steps; in the first step, LCM permutes operations to increase data reuse and the possibility of creating efficient codelets from that list of operations. This step also groups operations that have the most reuse into computation regions. The second step of LCM searches for efficient codelets in consecutive operations of each computation region.","title":"Inspector"}]}